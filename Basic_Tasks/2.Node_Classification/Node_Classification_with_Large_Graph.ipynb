{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GNN with Neighbor Sampling for Node Classification \n",
    "\n",
    "> 본 튜토리얼은 `GraphSAGE`를 사용하여 `ogbn-arxiv` 데이터를 기반으로 node classification을 수행합니다. 해당 데이터셋은 매우 큰 스케일의 데이터이기 때문에 Large Graph에서는 어떻게 node classification을 할 수 있을까에 대한 내용을 다루고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ogb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "import torch \n",
    "import numpy as np \n",
    "from ogb.nodeproppred import DglNodePropPredDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.08 GB: 100%|██████████| 81/81 [01:37<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\arxiv.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into DGL objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = DglNodePropPredDataset('ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=169343, num_edges=2332486,\n",
      "      ndata_schemes={'feat': Scheme(shape=(128,), dtype=torch.float32), 'year': Scheme(shape=(1,), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "tensor([[ 4],\n",
      "        [ 5],\n",
      "        [28],\n",
      "        ...,\n",
      "        [10],\n",
      "        [ 4],\n",
      "        [ 1]])\n",
      "Number of classes: 40\n"
     ]
    }
   ],
   "source": [
    "graph, node_labels = dataset[0]\n",
    "\n",
    "graph = dgl.add_reverse_edges(graph) # reverse_edges를 통해 directed graph를 undirected graph로 바꾸어 줍니다.\n",
    "graph.ndata['label'] = node_labels[:, 0] # label을 node 에 입력합니다.\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = dataset.get_idx_split() # index를 가지고와서 train, valid, test 셋을 분리합니다. \n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neighbor Sampler and Data Loader in DGL\n",
    "\n",
    "> `DGL`은 minibatch 형태로 사용할 수 있는 generater function을 제공하기 때문에 batch 단위의 데이터만 호출하여 모델을 학습할 수 있습니다. 이때 사용하는 함수가 바로 `dgl.dataloading.DataLoader`입니다. `DataLoader`는 PyTorch에서 사용하는 것과 동일한 방식으로 사용이 가능합니다. Graph에 존재하는 이웃을 Sampling하기 위해서는 `dgl.dataloading.NeighborSampler`를 사용하면 쉽게 Sampling이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.NeighborSampler([4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.NeighborSampler([4, 4]) # 4명의 이웃을 추출합니다. \n",
    "train_dataloader = dgl.dataloading.DataLoader(\n",
    "    # The following arguments are specific to DGL's DataLoader.\n",
    "    graph,              # The graph\n",
    "    train_nids,         # The node IDs to iterate over in minibatches\n",
    "    sampler,            # The neighbor sampler\n",
    "    device=device,      # Put the sampled MFGs on CPU or GPU\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=1024,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=False,    # Whether to drop the last incomplete batch\n",
    "    num_workers=0       # Number of sampler processes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 92312, 115166, 151892,  ...,  28142,  16181, 117105]), tensor([ 92312, 115166, 151892,  ..., 167075,  36614, 131277]), [Block(num_src_nodes=13039, num_dst_nodes=4116, num_edges=15033), Block(num_src_nodes=4116, num_dst_nodes=1024, num_edges=3299)]]\n",
      "To compute 1024 nodes' outputs, we need 13039 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, mfgs = example_minibatch = next(iter(train_dataloader)) # batch 단위에 들어간 정보를 출력할 수 있습니다. \n",
    "print(example_minibatch)\n",
    "print(\"To compute {} nodes' outputs, we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 92312, 115166, 151892,  ...,  28142,  16181, 117105])\n",
      "tensor([ 92312, 115166, 151892,  ...,  44630, 121947, 103380])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "mfg_0_src = mfgs[0].srcdata[dgl.NID] # NID는 고유한 node id를 가지고 오는 것입니다. src는 시작 node를 의미하고, dst는 도착 node를 의미합니다.\n",
    "mfg_0_dst = mfgs[0].dstdata[dgl.NID]\n",
    "print(mfg_0_src)\n",
    "print(mfg_0_dst)\n",
    "print(torch.equal(mfg_0_src[:mfgs[0].num_dst_nodes()], mfg_0_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean') # aggreator는 mean을 주로 사용합니다. (논문에서 mean을 사용합니다)\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        # Lines that are changed are marked with an arrow: \"<---\"\n",
    "\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()]  # <--- mfg는 message flow graph를 의미합니다. 주변 node들의 정보를 의미합니다. \n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  # <---\n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  # <---\n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  # <---\n",
    "        return h\n",
    "\n",
    "model = Model(num_features, 128, num_classes).to(device)\n",
    "\n",
    "# h = self.conv1(g, x)를 입력하게 되면 전체 노드에 대해 계산을 하는 것입니다. \n",
    "# 위 코드처럼 h = self.conv1(mfgs[0], (x, h_dst))를 사용하면 sampling된 node에 대해서만 계산하는 것을 의미합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.DataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt' # model parameter를 저장할 파일 이름을 의미합니다.\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "            # feature copy from CPU to GPU takes place here\n",
    "            inputs = mfgs[0].srcdata['feat']\n",
    "            labels = mfgs[-1].dstdata['label']\n",
    "\n",
    "            predictions = model(mfgs, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad(): # torch.no_grad()를 사용하여야 기울기를 계산하지 않습니다.\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            inputs = mfgs[0].srcdata['feat']\n",
    "            labels.append(mfgs[-1].dstdata['label'].cpu().numpy())\n",
    "            predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        # Note that this tutorial do not train the whole model to the end.\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dgl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91f4586c667beba9fc73d5e38cfe2361778c9000e218f655761c33977cb8e239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
