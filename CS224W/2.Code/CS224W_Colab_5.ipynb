{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS224W - Colab 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In this Colab we will experiment on scaling up GNNs using PyTorch Geometric, DeepSNAP and NetworkX. As we have **canceled** the Colab 5 assignment, this notebook will be a tutorial and you do not need to submit it on Gradescope.\n",
        "\n",
        "At first, we will use PyTorch Geometric `NeighborSampler` to scale up the training and testing on OGB `arxiv` dataset.\n",
        "\n",
        "Then, using the DeepSNAP and NetworkX, we will implement a simplified version of `NeighborSampler` and run experiments with different smapling ratios on the Cora graph.\n",
        "\n",
        "At last, we will partition the Cora graph into clusters by using different partition algorithms and then train the models in the way of vanilla Cluster-GCN.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_m9l6OYCQZP"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q ogb\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfgbfTjCRD_"
      },
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxkYLgxAOxz7"
      },
      "source": [
        "# 1 PyTorch Geometric Neighbor Sampling\n",
        "\n",
        "Neighbor Sampling, originally proposed in **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)), is a representative method to scale up GNNs. As what we have learned in lecture, only a K-hop neighborhood nodes will be loaded into GPU for each time training. To further reduce the cost, we can sample a subset of neighborhood nodes for GNNs to aggregate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kho6SHUVO1ny"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1WJLGKsOx_k"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqZWqRbO7km"
      },
      "source": [
        "## Neighbor Sampler\n",
        "\n",
        "PyTorch Geometric has implemented the Neighbor Sampling method as the [NeighborSampler](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.NeighborSampler) in `torch_geometric.data`. Following is an example that uses the Neighbor Sampling method on training the OGB `arxiv` dataset.\n",
        "\n",
        "If you are interested in memory-efficient aggregations, please refer to PyG's [Memory-Efficient Aggregations](https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWlyStlRO6_u"
      },
      "source": [
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)\n",
        "\n",
        "# Construct the training dataloader for training data\n",
        "# Sample 10 neighbors for each node in the first layer and 5 for the second layer\n",
        "train_loader = NeighborSampler(data.adj_t, node_idx=train_idx,\n",
        "                               sizes=[10, 5], batch_size=4096,\n",
        "                               shuffle=True, num_workers=2)\n",
        "\n",
        "# Specify size as -1 to include all neighbors\n",
        "all_loader = NeighborSampler(data.adj_t, node_idx=None, sizes=[-1],\n",
        "                                  batch_size=4096, shuffle=False,\n",
        "                                  num_workers=2)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjdkIcFpRYyl"
      },
      "source": [
        "## GNN Model\n",
        "\n",
        "After creating the `NeighborSampler`, we also need to modify the model to let it support the mini-batch training.\n",
        "\n",
        "The `forward` function will take the node feature `x` and a list of three-element tuples `adjs`. Each element in `adjs` contains following elements:\n",
        "* `edge_index`: The edge index tensor between source and destination nodes, which forms a bipartite grpah.\n",
        "* `e_id`: The indices of the edges in the original graph.\n",
        "* `size`: The shape of the bipartite graph, in (*number of source nodes*, *number of destination nodes*) format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRBJS_5qRWbu"
      },
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        for i in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "        self.convs.append(SAGEConv(hidden_dim, output_dim))\n",
        "\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adjs, mode=\"batch\"):\n",
        "        if mode == \"batch\":\n",
        "            for i, (edge_index, _, size) in enumerate(adjs):\n",
        "                # Extract target node features\n",
        "                x_target = x[:size[1]]\n",
        "\n",
        "                # Update x for next layer reuse\n",
        "                x = self.convs[i]((x, x_target), edge_index)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = self.bns[i](x)\n",
        "                    x = F.relu(x)\n",
        "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        else:\n",
        "            for i, conv in enumerate(self.convs):\n",
        "                x = conv(x, adjs)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = self.bns[i](x)\n",
        "                    x = F.relu(x)\n",
        "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.softmax(x)\n",
        "    \n",
        "    def inference(self, x_all, all_loader):\n",
        "        # This function will be called in test\n",
        "        for i in range(self.num_layers):\n",
        "            xs = []\n",
        "            for batch_size, n_id, adj in all_loader:\n",
        "                edge_index, _, size = adj.to(device)\n",
        "                x = x_all[n_id].to(device)\n",
        "                x_target = x[:size[1]]\n",
        "                x = self.convs[i]((x, x_target), edge_index)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = self.bns[i](x)\n",
        "                    x = F.relu(x)\n",
        "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                \n",
        "                # Append the node embeddings to xs\n",
        "                xs.append(x.cpu())\n",
        "            \n",
        "            # Concat all embeddings into one tensor\n",
        "            x_all = torch.cat(xs, dim=0)\n",
        "\n",
        "        return x_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cfm7K3wRqqY"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Now lets implement the training and testing functions.\n",
        "\n",
        "In both training and testing, we need to sample batch from the dataloader.\n",
        "\n",
        "Each batch in the `NeighborSampler` dataloader holds three elements:\n",
        "* `batch_size`: The batch size specified in the dataloader.\n",
        "* `n_id`: All nodes (in index format) used in the adjacency matrices.\n",
        "* `adjs`: The three-element tuples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JN0-_QCRn8N"
      },
      "source": [
        "def train(model, data, train_loader, train_idx, optimizer, loss_fn, mode=\"batch\"):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    if mode == \"batch\":\n",
        "        for batch_size, n_id, adjs in train_loader:\n",
        "            # Move all adj sparse tensors to GPU\n",
        "            adjs = [adj.to(device) for adj in adjs]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Index on the node features\n",
        "            out = model(data.x[n_id], adjs)\n",
        "            train_label = data.y[n_id[:batch_size]].squeeze(-1)\n",
        "            loss = loss_fn(out, train_label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "    else:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.adj_t, mode=mode)[train_idx]\n",
        "        train_label = data.y.squeeze(1)[train_idx]\n",
        "        loss = loss_fn(out, train_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss = loss.item()\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, all_loader, split_idx, evaluator, mode=\"batch\"):\n",
        "    model.eval()\n",
        "\n",
        "    if mode == \"batch\":\n",
        "        out = model.inference(data.x, all_loader)\n",
        "    else:\n",
        "        out = model(data.x, data.adj_t, mode=\"all\")\n",
        "\n",
        "    y_true = data.y.cpu()\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiehZ8OiR2q9"
      },
      "source": [
        "## Mini-batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFaI2eCARy0v"
      },
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 2,\n",
        "    'hidden_dim': 128,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}\n",
        "\n",
        "batch_model = SAGE(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "batch_model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(batch_model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_batch_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "batch_results = []\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    loss = train(batch_model, data, train_loader, train_idx, optimizer, loss_fn, mode=\"batch\")\n",
        "    result = test(batch_model, data, all_loader, split_idx, evaluator, mode=\"batch\")\n",
        "    batch_results.append(result)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_batch_model = copy.deepcopy(batch_model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        "best_result = test(best_batch_model, data, all_loader, split_idx, evaluator, mode=\"batch\")\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OyqW-1pSMLW"
      },
      "source": [
        "## Full-batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU5eAviTSFMO"
      },
      "source": [
        "# Use the same parameters for a full-batch training\n",
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 2,\n",
        "    'hidden_dim': 128,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}\n",
        "\n",
        "all_model = SAGE(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "all_model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(all_model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_all_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    loss = train(all_model, data, train_loader, train_idx, optimizer, loss_fn, mode=\"all\")\n",
        "    result = test(all_model, data, all_loader, split_idx, evaluator, mode=\"all\")\n",
        "    all_results.append(result)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_all_model = copy.deepcopy(all_model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        "best_result = test(best_all_model, data, all_loader, split_idx, evaluator, mode=\"all\")\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrECcOQQSZo1"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh_qvSG1SV63"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "batch_results = np.array(batch_results)\n",
        "all_results = np.array(all_results)\n",
        "\n",
        "x = np.arange(1, 101)\n",
        "\n",
        "plt.figure(figsize=(9, 7))\n",
        "\n",
        "plt.plot(x, batch_results[:, 1], label=\"Batch Validation\")\n",
        "plt.plot(x, batch_results[:, 2], label=\"Batch Test\")\n",
        "plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFb2OAvOSn_O"
      },
      "source": [
        "# 2 Neighbor Sampling with Different Ratios\n",
        "\n",
        "Now we will implement a simplified version of Neighbor Sampling by using DeepSNAP and NetworkX, and train models with different neighborhood sampling ratios.\n",
        "\n",
        "To make the experiments faster, we will use the Cora graph here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9U0F7bnSz9u"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUF4on-fSxcq"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from deepsnap.graph import Graph\n",
        "\n",
        "pyg_dataset = Planetoid('./tmp', \"Cora\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw6k-KdFTEYw"
      },
      "source": [
        "## GNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvUlNi2TS09i"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.dropout = args['dropout']\n",
        "        self.num_layers = args['num_layers']\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        for l in range(self.num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.post_mp = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data, mode=\"batch\"):\n",
        "        if mode == \"batch\":\n",
        "            edge_indices, x = data\n",
        "            for i in range(len(self.convs) - 1):\n",
        "                edge_index = edge_indices[i]\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_indices[len(self.convs) - 1])\n",
        "        else:\n",
        "            x, edge_index = data.node_feature, data.edge_index\n",
        "            for i in range(len(self.convs) - 1):\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_index)\n",
        "        x = self.post_mp(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulp1A3evcJ-I"
      },
      "source": [
        "## Neighbor Sampling\n",
        "\n",
        "Here we implement functions that will sample neighbors by using DeepSNAP and NetworkX.\n",
        "\n",
        "Notice that node classification task on Cora is a semi-supervised classification task, here we keep all the labeled training nodes (140 nodes) by setting the last ratio to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI4qHkE4cQOh"
      },
      "source": [
        "def sample_neighbors(nodes, G, ratio, all_nodes):\n",
        "    # This fuction takes a set of nodes, a NetworkX graph G and neighbor sampling ratio.\n",
        "    # It will return sampled neighbors (unioned with input nodes) and edges between \n",
        "    neighbors = set()\n",
        "    edges = []\n",
        "    for node in nodes:\n",
        "        neighbors_list = list(nx.neighbors(G, node))\n",
        "\n",
        "        # We only sample the (ratio * number of neighbors) neighbors\n",
        "        num = int(len(neighbors_list) * ratio)\n",
        "        if num > 0:\n",
        "            # Random shuffle the neighbors\n",
        "            random.shuffle(neighbors_list)\n",
        "            neighbors_list = neighbors_list[:num]\n",
        "            for neighbor in neighbors_list:\n",
        "                # Add neighbors\n",
        "                neighbors.add(neighbor)\n",
        "                edges.append((neighbor, node))\n",
        "    return neighbors, neighbors.union(all_nodes), edges\n",
        "\n",
        "def nodes_to_tensor(nodes):\n",
        "    # This function transform a set of nodes to node index tensor\n",
        "    node_label_index = torch.tensor(list(nodes), dtype=torch.long)\n",
        "    return node_label_index\n",
        "\n",
        "def edges_to_tensor(edges):\n",
        "    # This function transform a set of edges to edge index tensor\n",
        "    edge_index = torch.tensor(list(edges), dtype=torch.long)\n",
        "    edge_index = torch.cat([edge_index, torch.flip(edge_index, [1])], dim=0)\n",
        "    edge_index = edge_index.permute(1, 0)\n",
        "    return edge_index\n",
        "\n",
        "def relable(nodes, labeled_nodes, edges_list):\n",
        "    # Relable the nodes, labeled_nodes and edges_list\n",
        "    relabled_edges_list = []\n",
        "    sorted_nodes = sorted(nodes)\n",
        "    node_mapping = {node : i for i, node in enumerate(sorted_nodes)}\n",
        "    for orig_edges in edges_list:\n",
        "        relabeled_edges = []\n",
        "        for edge in orig_edges:\n",
        "            relabeled_edges.append((node_mapping[edge[0]], node_mapping[edge[1]]))\n",
        "        relabled_edges_list.append(relabeled_edges)\n",
        "    relabeled_labeled_nodes = [node_mapping[node] for node in labeled_nodes]\n",
        "    relabeled_nodes = [node_mapping[node] for node in nodes]\n",
        "    return relabled_edges_list, relabeled_nodes, relabeled_labeled_nodes\n",
        "\n",
        "def neighbor_sampling(graph, K=2, ratios=(0.1, 0.1, 0.1)):\n",
        "    # This function takes a DeepSNAP graph, K the number of GNN layers, and neighbor \n",
        "    # sampling ratios for each layer. This function returns relabeled node feature, \n",
        "    # edge indices and node_label_index\n",
        "\n",
        "    assert K + 1 == len(ratios)\n",
        "\n",
        "    labeled_nodes = graph.node_label_index.tolist()\n",
        "    random.shuffle(labeled_nodes)\n",
        "    num = int(len(labeled_nodes) * ratios[-1])\n",
        "    if num > 0:\n",
        "        labeled_nodes = labeled_nodes[:num]\n",
        "    nodes_list = [set(labeled_nodes)]\n",
        "    edges_list = []\n",
        "    all_nodes = labeled_nodes\n",
        "    for k in range(K):\n",
        "        # Get nodes and edges from the previous layer\n",
        "        nodes, all_nodes, edges = \\\n",
        "            sample_neighbors(nodes_list[-1], graph.G, ratios[len(ratios) - k - 2], all_nodes)\n",
        "        nodes_list.append(nodes)\n",
        "        edges_list.append(edges)\n",
        "    \n",
        "    # Reverse the lists\n",
        "    nodes_list.reverse()\n",
        "    edges_list.reverse()\n",
        "\n",
        "    relabled_edges_list, relabeled_all_nodes, relabeled_labeled_nodes = \\\n",
        "        relable(all_nodes, labeled_nodes, edges_list)\n",
        "\n",
        "    node_index = nodes_to_tensor(relabeled_all_nodes)\n",
        "    # All node features that will be used\n",
        "    node_feature = graph.node_feature[node_index]\n",
        "    edge_indices = [edges_to_tensor(edges) for edges in relabled_edges_list]\n",
        "    node_label_index = nodes_to_tensor(relabeled_labeled_nodes)\n",
        "    log = \"Sampled {} nodes, {} edges, {} labeled nodes\"\n",
        "    print(log.format(node_feature.shape[0], edge_indices[0].shape[1] // 2, node_label_index.shape[0]))\n",
        "    return node_feature, edge_indices, node_label_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooy6Hcf7TIhI"
      },
      "source": [
        "## Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSmZhpzPTGPY"
      },
      "source": [
        "def train(train_graphs, val_graphs, args, model, optimizer, mode=\"batch\"):\n",
        "    best_val = 0\n",
        "    best_model = None\n",
        "    accs = []\n",
        "    graph_train = train_graphs[0]\n",
        "    graph_train.to(args['device'])\n",
        "    for epoch in range(1, 1 + args['epochs']):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if mode == \"batch\":\n",
        "            node_feature, edge_indices, node_label_index = neighbor_sampling(graph_train, args['num_layers'], args['ratios'])\n",
        "            node_feature = node_feature.to(args['device'])\n",
        "            node_label_index = node_label_index.to(args['device'])\n",
        "            for i in range(len(edge_indices)):\n",
        "                edge_indices[i] = edge_indices[i].to(args['device'])\n",
        "            pred = model([edge_indices, node_feature])\n",
        "            pred = pred[node_label_index]\n",
        "            label = graph_train.node_label[node_label_index]\n",
        "        elif mode == \"community\":\n",
        "            graph = random.choice(train_graphs)\n",
        "            graph = graph.to(args['device'])\n",
        "            pred = model(graph, mode=\"all\")\n",
        "            pred = pred[graph.node_label_index]\n",
        "            label = graph.node_label[graph.node_label_index]\n",
        "        else:\n",
        "            pred = model(graph_train, mode=\"all\")\n",
        "            label = graph_train.node_label\n",
        "            pred = pred[graph_train.node_label_index]\n",
        "        loss = F.nll_loss(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc, val_acc, test_acc = test(val_graphs, model)\n",
        "        accs.append((train_acc, val_acc, test_acc))\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_model = copy.deepcopy(model)\n",
        "        print(f'Epoch: {epoch:02d}, '\n",
        "              f'Loss: {loss:.4f}, '\n",
        "              f'Train: {100 * train_acc:.2f}%, '\n",
        "              f'Valid: {100 * val_acc:.2f}% '\n",
        "              f'Test: {100 * test_acc:.2f}%')\n",
        "    return best_model, accs\n",
        "\n",
        "def test(graphs, model):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    for graph in graphs:\n",
        "        graph = graph.to(args['device'])\n",
        "        pred = model(graph, mode=\"all\")\n",
        "        label = graph.node_label\n",
        "        pred = pred[graph.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(label).sum().item()\n",
        "        acc /= len(label)\n",
        "        accs.append(acc)\n",
        "    return accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV7i0v0ETKzf"
      },
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'dropout': 0.5,\n",
        "    'num_layers': 2,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 50,\n",
        "    'ratios': (0.8, 0.8, 1),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLpRYKbnTQnj"
      },
      "source": [
        "## Full-Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGGjbJBTOo1"
      },
      "source": [
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "\n",
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "graphs = [graph_train, graph_val, graph_test]\n",
        "all_best_model, all_accs = train(graphs, graphs, args, model, optimizer, mode=\"all\")\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], all_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWkGiwB6Thr4"
      },
      "source": [
        "## Sampling with Ratios 0.8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWusJ9u3Tfhv"
      },
      "source": [
        "args['ratios'] = (0.8, 0.8, 1)\n",
        "\n",
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "\n",
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "graphs = [graph_train, graph_val, graph_test]\n",
        "batch_best_model, batch_accs = train(graphs, graphs, args, model, optimizer)\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], batch_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_FjkNHDT4c6"
      },
      "source": [
        "## Sampling with Ratios 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "booJ6DASTjO4"
      },
      "source": [
        "# Change the ratio to 0.3\n",
        "args['ratios'] = (0.3, 0.3, 1)\n",
        "\n",
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "\n",
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "graphs = [graph_train, graph_val, graph_test]\n",
        "batch_best_model, batch_accs_1 = train(graphs, graphs, args, model, optimizer)\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], batch_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EePAvNlGUM2K"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7etNAkXAT55d"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "batch_results = np.array(batch_accs)\n",
        "batch_results_1 = np.array(batch_accs_1)\n",
        "all_results = np.array(all_accs)\n",
        "\n",
        "x = np.arange(1, 51)\n",
        "\n",
        "plt.figure(figsize=(9, 7))\n",
        "\n",
        "plt.plot(x, batch_results[:, 0], label=\"Batch 0.8 Train\")\n",
        "plt.plot(x, batch_results[:, 1], label=\"Batch 0.8 Validation\")\n",
        "plt.plot(x, batch_results[:, 2], label=\"Batch 0.8 Test\")\n",
        "plt.plot(x, batch_results_1[:, 0], label=\"Batch 0.3 Train\")\n",
        "plt.plot(x, batch_results_1[:, 1], label=\"Batch 0.3 Validation\")\n",
        "plt.plot(x, batch_results_1[:, 2], label=\"Batch 0.3 Test\")\n",
        "plt.plot(x, all_results[:, 0], label=\"All Train\")\n",
        "plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkA7-0groq7q"
      },
      "source": [
        "Here all accuracies are evaluated on the full-batch mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iee0U8KGURc8"
      },
      "source": [
        "# 3 Cluster Sampling\n",
        "\n",
        "Instead of the neighbor sampling, we can use another approach, subgraph (cluster) sampling, to scale up GNN. This approach is proposed in Cluster-GCN ([Chiang et al. (2019)](https://arxiv.org/abs/1905.07953)).\n",
        "\n",
        "In this section, we will implement vanilla Cluster-GCN and experiment with 3 different community partition algorithms.\n",
        "\n",
        "Notice that this section requires you have run the `Setup`, `GNN Model` and `Training and Testing` cells of the last section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BXjP79gUYir"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQ_VKp8UOEm"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import community as community_louvain\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from deepsnap.graph import Graph\n",
        "\n",
        "pyg_dataset = Planetoid('./tmp', \"Cora\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMatyCSUaB6"
      },
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'dropout': 0.5,\n",
        "    'num_layers': 2,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 150,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekV-sokSUeLc"
      },
      "source": [
        "## Partition the Graph into Clusters\n",
        "\n",
        "Here we use following three community detection / partition algorithms to partition the graph into different clusters:\n",
        "* [Kernighan–Lin algorithm (bisection)](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection.html)\n",
        "* [Clauset-Newman-Moore greedy modularity maximization](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities)\n",
        "* [Louvain algorithm](https://python-louvain.readthedocs.io/en/latest/api.html)\n",
        "\n",
        "\n",
        "To make the training more stable, we discard the cluster that has less than 10 nodes.\n",
        "\n",
        "Let's first define these algorithms as DeepSNAP transformation on a graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8XeT005UcKh"
      },
      "source": [
        "def preprocess(G, node_label_index, method=\"louvain\"):\n",
        "    graphs = []\n",
        "    labeled_nodes = set(node_label_index.tolist())\n",
        "    if method == \"louvain\":\n",
        "        community_mapping = community_louvain.best_partition(G, resolution=10)\n",
        "        communities = {}\n",
        "        for node in community_mapping:\n",
        "            comm = community_mapping[node]\n",
        "            if comm in communities:\n",
        "                communities[comm].add(node)\n",
        "            else:\n",
        "                communities[comm] = set([node])\n",
        "        communities = communities.values()\n",
        "    elif method == \"bisection\":\n",
        "        communities = nx.algorithms.community.kernighan_lin_bisection(G)\n",
        "    elif method == \"greedy\":\n",
        "        communities = nx.algorithms.community.greedy_modularity_communities(G)\n",
        "\n",
        "    for community in communities:\n",
        "        nodes = set(community)\n",
        "        subgraph = G.subgraph(nodes)\n",
        "        # Make sure each subgraph has more than 10 nodes\n",
        "        if subgraph.number_of_nodes() > 10:\n",
        "            node_mapping = {node : i for i, node in enumerate(subgraph.nodes())}\n",
        "            subgraph = nx.relabel_nodes(subgraph, node_mapping)\n",
        "            # Get the id of the training set labeled node in the new graph\n",
        "            train_label_index = []\n",
        "            for node in labeled_nodes:\n",
        "                if node in node_mapping:\n",
        "                    # Append relabeled labeled node index\n",
        "                    train_label_index.append(node_mapping[node])\n",
        "\n",
        "            # Make sure the subgraph contains at least one training set labeled node\n",
        "            if len(train_label_index) > 0:\n",
        "                dg = Graph(subgraph)\n",
        "                # Update node_label_index\n",
        "                dg.node_label_index = torch.tensor(train_label_index, dtype=torch.long)\n",
        "                graphs.append(dg)\n",
        "    return graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CYEamCAU-TJ"
      },
      "source": [
        "## Louvain Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TrC6ybWU7eO"
      },
      "source": [
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"louvain\")\n",
        "print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "avg_num_nodes = 0\n",
        "avg_num_edges = 0\n",
        "for graph in graphs:\n",
        "    avg_num_nodes += graph.num_nodes\n",
        "    avg_num_edges += graph.num_edges\n",
        "avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O03uXIuGVIgJ"
      },
      "source": [
        "## Louvain Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSbGf5ADVFQq"
      },
      "source": [
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "louvain_best_model, louvain_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], louvain_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CvTf0ANVO9U"
      },
      "source": [
        "## Bisection Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkV0zlhgVJ7u"
      },
      "source": [
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"bisection\")\n",
        "print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "avg_num_nodes = 0\n",
        "avg_num_edges = 0\n",
        "for graph in graphs:\n",
        "    avg_num_nodes += graph.num_nodes\n",
        "    avg_num_edges += graph.num_edges\n",
        "avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqMCvP8wVVms"
      },
      "source": [
        "## Bisection Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1wgFg1bVRGY"
      },
      "source": [
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "bisection_best_model, bisection_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], bisection_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PROPwoOVcJy"
      },
      "source": [
        "## Greedy Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3DVamWqVT92"
      },
      "source": [
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"greedy\")\n",
        "print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "avg_num_nodes = 0\n",
        "avg_num_edges = 0\n",
        "for graph in graphs:\n",
        "    avg_num_nodes += graph.num_nodes\n",
        "    avg_num_edges += graph.num_edges\n",
        "avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93pR_-kxVgma"
      },
      "source": [
        "## Greedy Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQgQY-jPVd_U"
      },
      "source": [
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "greedy_best_model, greedy_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], greedy_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5edKKT6Vk1C"
      },
      "source": [
        "## Full-Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5tIXxC8ViFD"
      },
      "source": [
        "graphs_train, graphs_val, graphs_test = \\\n",
        "    GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "graph_train = graphs_train[0]\n",
        "graph_val = graphs_val[0]\n",
        "graph_test = graphs_test[0]\n",
        "\n",
        "model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "graphs = [graph_train, graph_val, graph_test]\n",
        "all_best_model, all_accs = train(graphs, graphs, args, model, optimizer, mode=\"all\")\n",
        "train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], all_best_model)\n",
        "print('Best model:',\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * val_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpuETv7Vpx0"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMK33kY5VmF5"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "louvain_results = np.array(louvain_accs)\n",
        "bisection_results = np.array(bisection_accs)\n",
        "greedy_results = np.array(greedy_accs)\n",
        "all_results = np.array(all_accs)\n",
        "\n",
        "x = np.arange(1, 151)\n",
        "\n",
        "plt.figure(figsize=(9, 7))\n",
        "\n",
        "plt.plot(x, louvain_results[:, 1], label=\"Louvain Validation\")\n",
        "plt.plot(x, louvain_results[:, 2], label=\"Louvain Test\")\n",
        "plt.plot(x, bisection_results[:, 1], label=\"Bisection Validation\")\n",
        "plt.plot(x, bisection_results[:, 2], label=\"Bisection Test\")\n",
        "plt.plot(x, greedy_results[:, 1], label=\"Greedy Validation\")\n",
        "plt.plot(x, greedy_results[:, 2], label=\"Greedy Test\")\n",
        "plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}