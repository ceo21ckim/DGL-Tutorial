{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224W - Colab 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In this Colab, we will shift our focus from homogenous graphs to heterogeneous graphs. Heterogeneous graphs extend the traditional homogenous graphs that we have seen before by specifically incorperating different node and edge types. This additional information allows us to extend the graph neural nework models that we have worked with before. Namely, we can apply heterogenous message passing, where different message types now exist between different node, edge type relationships. \n",
        "\n",
        "At first, we will learn how to transform NetworkX graphs into DeepSNAP representations. Additionally, we will dive deeper into how DeepSNAP stores and represents heterogeneous graphs as PyTorch Tensors.\n",
        "\n",
        "Then, we will build our own heterogenous graph neural netowrk models using PyTorch Geonetric and DeepSNAP on node property prediction task. To evaluate these models, we will use our model on the heterogeneous ACM dataset.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n",
        "\n",
        "Have fun on Colab 4 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_m9l6OYCQZP"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfgbfTjCRD_"
      },
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoXlf4MtYrbz"
      },
      "source": [
        "# 1 DeepSNAP Heterogeneous Graph\n",
        "\n",
        "First, we will explore how to transform NetworkX graphs into the format supported by DeepSNAP. \n",
        "\n",
        "In order to extend the traditional DeepSNAP representation of graphs to include heterogenous graphs, we include the following graph property features:\n",
        "* `node_feature`: The feature of each node (`torch.tensor`)\n",
        "* `edge_feature`: The feautre of each edge (`torch.tensor`)\n",
        "* `node_label`: The label of each node (`int`)\n",
        "* `node_type`: The node type of each node (`string`)\n",
        "* `edge_type`: The edge type of each edge (`string`)\n",
        "\n",
        "The key new features we add are `node_type` and `edge_type`, which enables us to perform heterogenous message passing.\n",
        "\n",
        "In this first question we will work with the familiar [karate club graph](https://networkx.github.io/documentation/stable/auto_examples/graph/plot_karate_club.html) seen in assignment one. To start, since each node in the graph belongs to one of two clubs (club \"Mr. Hi\" or club \"Officer\"), we will treat the club as the `node_type`. The code below demonstrates how to differentiate the nodes in the NetworkX graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LQ_z5gcBVA1"
      },
      "source": [
        "from pylab import *\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "G = nx.karate_club_graph()\n",
        "community_map = {}\n",
        "for node in G.nodes(data=True):\n",
        "  if node[1][\"club\"] == \"Mr. Hi\":\n",
        "    community_map[node[0]] = 0\n",
        "  else:\n",
        "    community_map[node[0]] = 1\n",
        "node_color = []\n",
        "color_map = {0: 0, 1: 1}\n",
        "node_color = [color_map[community_map[node]] for node in G.nodes()]\n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize=(7, 7))\n",
        "nx.draw(G, pos=pos, cmap=plt.get_cmap('coolwarm'), node_color=node_color)\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JpFb9fTw1lg"
      },
      "source": [
        "### Question 1.1: Assigning Node Type and Node Features (Not Specifically Graded)\n",
        "\n",
        "Using the `community_map` dictionary and graph `G` from above, add node attributes `node_type` and `node_label` to the graph G. Namely, for `node_type` assign nodes in the \"Mr. Hi\" club to a node type `n0` and nodes in club \"Officer\" a node type `n1`. \n",
        "\n",
        "Then for `node_label`, assign nodes in \"Mr. Hi\" club to a `node_label` `0` and nodes in club \"Officer\" a `node_label` of `1`.\n",
        "\n",
        "Lastly, assign every node a feature vector [1, 1, 1, 1, 1]. \n",
        "\n",
        "**Hint**: Look at the NetworkX function `nx.classes.function.set_node_attributes`.\n",
        "\n",
        "**Note**: This question is not specifically graded but is important for later questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zev_hMJHJXK1"
      },
      "source": [
        "import torch\n",
        "\n",
        "def assign_node_types(G, community_map):\n",
        "  # TODO: Implement this function that takes in a NetworkX graph\n",
        "  # G and community map assignment (mapping node ids --> 0/1 labels)\n",
        "  # and adds 'node_type' as a node_attribute in G.\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~2 line of code)\n",
        "  ## Note\n",
        "  ## 1. Look up NetworkX `nx.classes.function.set_node_attributes`\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "def assign_node_labels(G, community_map):\n",
        "  # TODO: Implement this function that takes in a NetworkX graph\n",
        "  # G and community map assignment (mapping node ids --> 0/1 labels)\n",
        "  # and adds 'node_label' as a node_attribute in G.\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~2 line of code)\n",
        "  ## Note\n",
        "  ## 1. Look up NetworkX `nx.classes.function.set_node_attributes`\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "def assign_node_features(G):\n",
        "  # TODO: Implement this function that takes in a NetworkX graph\n",
        "  # G and adds 'node_feature' as a node_attribute in G. Each node\n",
        "  # in the graph has the same feature vector [1., 1., 1., 1., 1.]\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~2 line of code)\n",
        "  ## Note\n",
        "  ## 1. Look up NetworkX `nx.classes.function.set_node_attributes`\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "assign_node_types(G, community_map)\n",
        "assign_node_labels(G, community_map)\n",
        "assign_node_features(G)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mafN0P3EOhSb"
      },
      "source": [
        "### Question 1.2: Assigning Edge Types (Not Specifically Graded)\n",
        "\n",
        "Next, we will assign three different types of `edge_type` to the edges: \n",
        "* Edges within club \"Mr. Hi\": `e0`\n",
        "* Edges within club \"Officer\": `e1`\n",
        "* Edges between clubs: `e2`\n",
        "\n",
        "**Hint**: Use the `community_map` from before and `nx.classes.function.set_edge_attributes`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsbYWEVwSV5n"
      },
      "source": [
        "def assign_edge_types(G, community_map):\n",
        "  # TODO: Implement this function that takes in a NetworkX graph\n",
        "  # G and community map assignment (mapping node ids --> 0/1 labels)\n",
        "  # and adds 'edge_type' as a edge_attribute in G.\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~5 line of code)\n",
        "  ## Note\n",
        "  ## 1. Create an edge assignment dict following rules above\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "assign_edge_types(G, community_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBsTdTPVTQ52"
      },
      "source": [
        "## Heterogeneous Graph Visualization\n",
        "\n",
        "Now we can visualize the Heterogeneous Graph we have generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2sdufbODHtp"
      },
      "source": [
        "edge_color = {}\n",
        "for edge in G.edges():\n",
        "  n1, n2 = edge\n",
        "  edge_color[edge] = community_map[n1] if community_map[n1] == community_map[n2] else 2\n",
        "  if community_map[n1] == community_map[n2] and community_map[n1] == 0:\n",
        "    edge_color[edge] = 'blue'\n",
        "  elif community_map[n1] == community_map[n2] and community_map[n1] == 1:\n",
        "    edge_color[edge] = 'red'\n",
        "  else:\n",
        "    edge_color[edge] = 'green'\n",
        "\n",
        "G_orig = copy.deepcopy(G)\n",
        "nx.classes.function.set_edge_attributes(G, edge_color, name='color')\n",
        "colors = nx.get_edge_attributes(G,'color').values()\n",
        "labels = nx.get_node_attributes(G, 'node_type')\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(G, pos=pos, cmap=plt.get_cmap('coolwarm'), node_color=node_color, edge_color=colors, labels=labels, font_color='white')\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRANkF1jRxLV"
      },
      "source": [
        "which include edges within each clubs (2 types) and edges across two clubs (1 type). Different types of nodes and edges are visualized in different colors. The NetworkX object `G` in following code can be transformed into `deepsnap.hetero_graph.HeteroGraph` directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DW8L0hxbxw4"
      },
      "source": [
        "## Transforming to DeepSNAP representation\n",
        "\n",
        "The NetworkX object `G` in following code can be transformed into `deepsnap.hetero_graph.HeteroGraph` directly!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZfHZ_eoVVGd"
      },
      "source": [
        "from deepsnap.hetero_graph import HeteroGraph\n",
        "\n",
        "hete = HeteroGraph(G_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izq4t_O9WxDH"
      },
      "source": [
        "## Question 1.3: How many nodes are of each type (10 Points)\n",
        "\n",
        "Submit your answers on Gradescope.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRNSP6nnW78C"
      },
      "source": [
        "def get_nodes_per_type(hete):\n",
        "  # TODO: Implement this function that takes a DeepSNAP dataset object\n",
        "  # and return the number of nodes per `node_type`.\n",
        "\n",
        "  num_nodes_n0 = 0\n",
        "  num_nodes_n1 = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~2 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful.\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return num_nodes_n0, num_nodes_n1\n",
        "\n",
        "num_nodes_n0, num_nodes_n1 = get_nodes_per_type(hete)\n",
        "print(\"Node type n0 has {} nodes\".format(num_nodes_n0))\n",
        "print(\"Node type n1 has {} nodes\".format(num_nodes_n1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsHJp2ZYaE2"
      },
      "source": [
        "## Question 1.4: Message Types: How many edges are of each message type (10 Points)\n",
        "\n",
        "Submit your answers on Gradescope.\n",
        "\n",
        "When working with heterogenous graphs, as we have discussed before, we now work with heterogenous message types (i.e. different message types for `node_type` and `edge_type` combinations). For example, an edge of type `e0` connecting two nodes in club \"Mr. HI\" would have a message type of (`n0`, `e0`, `n0`). In this problem we will analyze how many edges in our graph are of each message type.\n",
        "\n",
        "**Hint**: If you want to learn more about what the different message types are try the call `hete.message_types`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qobKuqbAYvJ7"
      },
      "source": [
        "def get_num_message_edges(hete):\n",
        "  # TODO: Implement this function that takes a DeepSNAP dataset object\n",
        "  # and return the number of edges for each message type. \n",
        "  # You should return a list of tuples as \n",
        "  # (message_type, num_edge)\n",
        "\n",
        "  message_type_edges = []\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~2 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful.\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return message_type_edges\n",
        "\n",
        "message_type_edges = get_num_message_edges(hete)\n",
        "for (message_type, num_edges) in message_type_edges:\n",
        "  print(\"Message type {} has {} edges\".format(message_type, num_edges))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjMVik1JbJ76"
      },
      "source": [
        "## Question 1.5: Dataset Splitting: How many nodes in each dataset split? (10 Points)\n",
        "\n",
        "Submit your answers on Gradescope.\n",
        "\n",
        "DeepSNAP has built in Dataset creation and splitting methods for heterogeneous graphs. Here we will create train, validation, and test datasets for a node prediction task and inspect the resulting subgraphs. Specifically, write a function that computes the number of nodes in each dataset split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct10Oh4gcqgD"
      },
      "source": [
        "from deepsnap.dataset import GraphDataset\n",
        "\n",
        "def compute_dataset_split_counts(datasets):\n",
        "  # TODO: Implement this function that takes a dict of datasets\n",
        "  # and returns a dict mapping dataset names to the number of labeled\n",
        "  # nodes used for supervision in that respective dataset.  \n",
        "  \n",
        "  data_set_splits = {}\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~3 line of code)\n",
        "  ## Note\n",
        "  ## 1. DeepSNAP `node_label_index` will be helpful.\n",
        "  ## 2. Remember to count both node_types\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return data_set_splits\n",
        "\n",
        "\n",
        "dataset = GraphDataset([hete], task='node')\n",
        "# Splitting the dataset\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
        "datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
        "\n",
        "data_set_splits = compute_dataset_split_counts(datasets)\n",
        "for dataset_name, num_nodes in data_set_splits.items():\n",
        "  print(\"{} dataset has {} nodes\".format(dataset_name, num_nodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFY2PaDbVKe4"
      },
      "source": [
        "## DeepSNAP Dataset Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiyEw-agbgV8"
      },
      "source": [
        "from deepsnap.dataset import GraphDataset\n",
        "\n",
        "dataset = GraphDataset([hete], task='node')\n",
        "# Splitting the dataset\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
        "titles = ['Train', 'Validation', 'Test']\n",
        "\n",
        "for i, dataset in enumerate([dataset_train, dataset_val, dataset_test]):\n",
        "  n0 = hete._convert_to_graph_index(dataset[0].node_label_index['n0'], 'n0').tolist()\n",
        "  n1 = hete._convert_to_graph_index(dataset[0].node_label_index['n1'], 'n1').tolist()\n",
        "\n",
        "  plt.figure(figsize=(7, 7))\n",
        "  plt.title(titles[i])\n",
        "  nx.draw(G_orig, pos=pos, node_color=\"grey\", edge_color=colors, labels=labels, font_color='white')\n",
        "  nx.draw_networkx_nodes(G_orig.subgraph(n0), pos=pos, node_color=\"blue\")\n",
        "  nx.draw_networkx_nodes(G_orig.subgraph(n1), pos=pos, node_color=\"red\")\n",
        "  show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5LsVSRuI3hU"
      },
      "source": [
        "# 2 Heterogeneous Graph Node Property Prediction\n",
        "\n",
        "In this part we will use PyTorch Geometric and DeepSNAP to implement a GNN model for heterogeneous graph node property prediction (node classification). This part of Colab requires you having good understandings on the heterogeneous graph and how to implement the GNN layers by using PyG.\n",
        "\n",
        "At first let's take look at the general structure of a heterogeneous layer by an example.\n",
        "\n",
        "Let's assume we have a graph $G$, which contains two node types $a$ and $b$, and three message types $m_1=(a, r_1, a)$, $m_2=(a, r_2, b)$ and $m_3=(a, r_3, b)$.\n",
        "\n",
        "Thus, for $G$ a heterogeneous layer will contains three Heterogeneous GNN layers (`HeteroGNNConv` in this Colab) where each `HeteroGNNConv` layer will perform the message passing and aggregation with respect to only one message type. The overview of the heterogeneous layer is shown below:\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img src=\"https://web.stanford.edu/class/cs224w/images/colab4/hetero_conv.png\"/>\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "In this Colab, all the $l^{th}$ Heterogeneous GNN layers will be managed by a ($l^{th}$) Heterogeneous GNN Wrapper layer (the `HeteroGNNWrapperConv`). The $l^{th}$ Heterogeneous GNN Wrapper layer will take in the input node embeddings from $(l-1)^{th}$ layer and aggregate (across message types) the Heterogeneous GNN layers' results. For example, the wrapper layer will aggregate node type $b$'s node embeddings from Heterogeneous GNN layers for $m_2$ and $m_3$. The \"simplified\" heterogeneous layer structure is shown below:\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img src=\"http://web.stanford.edu/class/cs224w/images/colab4/hetero_conv_1.png\"/>\n",
        "</center>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOTCyuRcJikS"
      },
      "source": [
        "<font color='red'>We recommend you implement the heterogeneous GNN model in following steps:</font>\n",
        "\n",
        "1. Implement the `HeteroGNNConv` first.\n",
        "2. Implement the `mean` aggregation in `HeteroGNNWrapperConv`.\n",
        "3. Implement the `generate_convs`.\n",
        "4. Implement the `HeteroGNN` model and the `train` function.\n",
        "5. Train the model with `mean` aggregation across the message types and make sure your model has reasonable performance.\n",
        "6. Implement the `attn` aggregation in `HeteroGNNWrapperConv`.\n",
        "7. Train the model with `attn` aggregation across the message types and make sure your model has reasonable performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkFjcktiJJLm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAm9_OcJJJ-W"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import deepsnap\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from deepsnap.hetero_gnn import forward_op\n",
        "from deepsnap.hetero_graph import HeteroGraph\n",
        "from torch_sparse import SparseTensor, matmul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2prITo3JSbo"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "You need to login to your Google account and enter the verification code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvQwxJX4JTJX"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igoy4F_xJbVn"
      },
      "source": [
        "id='1ivlxd6lJMcZ9taS44TMGG72x2V1GeVvk'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('acm.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBlboS5kJmJL"
      },
      "source": [
        "## Heterogeneous GNN Layer\n",
        "\n",
        "Now let's start working on our own implementation of a heterogeneous layer (the `HeteroGNNConv`)! Similar to what we did in Colab 3, we will implement the layer using PyTorch Geometric. In general, our heterogeneous GNN layer draws ideas from the **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)).\n",
        "\n",
        "At first, let's implement the GNN layer for each message type :\n",
        "\n",
        "\\begin{equation}\n",
        "m =(s, r, d)\n",
        "\\end{equation}\n",
        "\n",
        "Each message type is a tuple containing three elements where $s$ refers to the source node type, $r$ refers to the edge (relation) type and $d$ refers to the destination node type. The update rule is very similar to that of GraphSAGE but we need to include the node types and the edge type. The update rule is described as below:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)[m]} = W^{(l)[m]} \\cdot \\text{CONCAT} \\Big( W_d^{(l)[m]} \\cdot h_v^{(l-1)}, W_s^{(l)[m]} \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N_{m}(v) \\})\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $[m]$ indicates that the weight matrices or embeddings with respect to message type $m$, $W_s^{(l)[m]}$ computes the messages from neighboring nodes, $W_d^{(l)[m]}$ compute messages from the node itself, and $W^{(l)[m]}$ aggregates messages from both node types. In the equation above, $v$ has the node type $d$, and $u$ has the node type $s$.\n",
        "\n",
        "For simplicity, we use mean aggregations for $AGG$ where:\n",
        "\n",
        "\\begin{equation}\n",
        "AGG(\\{h_u^{(l-1)}, \\forall u \\in N_{m}(v) \\}) = \\frac{1}{|N_{m}(v)|} \\sum_{u\\in N_{m}(v)} h_u^{(l-1)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z1b0Mf8Jova"
      },
      "source": [
        "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
        "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
        "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
        "\n",
        "        self.in_channels_src = in_channels_src\n",
        "        self.in_channels_dst = in_channels_dst\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # To simplify implementation, please initialize both self.lin_dst\n",
        "        # and self.lin_src out_features to out_channels\n",
        "        self.lin_dst = None\n",
        "        self.lin_src = None\n",
        "\n",
        "        self.lin_update = None\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~3 lines of code)\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        node_feature_src,\n",
        "        node_feature_dst,\n",
        "        edge_index,\n",
        "        size=None,\n",
        "        res_n_id=None,\n",
        "    ):\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note:\n",
        "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
        "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
        "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
        "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
        "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~4 lines of code)\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "        return aggr_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKq8ScTiJthn"
      },
      "source": [
        "## Heterogeneous GNN Wrapper Layer\n",
        "\n",
        "After implementing the GNN layer for each message type, we need to somehow aggregate the the node embedding results (with respect to each message types) together. Here we will implement two types of message type level aggregation.\n",
        "\n",
        "The first one is simply the mean aggregation:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = \\frac{1}{M}\\sum_{m=1}^{M}h_v^{(l)[m]}\n",
        "\\end{equation}\n",
        "\n",
        "Here node $v$ has the node type $d$ and $M$ is the total number of message types that the destination node type is $d$.\n",
        "\n",
        "The other one is the semantic level attention introduced in **HAN** ([Wang et al. (2019)](https://arxiv.org/abs/1903.07293)). Instead of directly averaging on the message type aggregation results, we use attention to learn which message type result can be more important, then aggregate from all the message types. Following are the equations for semantic level attention:\n",
        "\n",
        "\\begin{equation}\n",
        "e_{m} = \\frac{1}{|V_{d}|} \\sum_{v \\in V_{d}} q_{attn}^T \\cdot tanh \\Big( W_{attn}^{(l)} \\cdot h_v^{(l)[m]} + b \\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $m$ refers to message type and $d$ refers to the destination node type. Then we can compute the attention and update the $h_v^{(l)}$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_{m} = \\frac{\\exp(e_{m})}{\\sum_{m=1}^M \\exp(e_{m})}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = \\sum_{m=1}^{M} \\alpha_{m} \\cdot h_v^{(l)[m]}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_bun02xJwFm"
      },
      "source": [
        "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
        "    def __init__(self, convs, args, aggr=\"mean\"):\n",
        "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
        "        self.aggr = aggr\n",
        "\n",
        "        # Map the index and message type\n",
        "        self.mapping = {}\n",
        "\n",
        "        # A numpy array that stores the final attention probability\n",
        "        self.alpha = None\n",
        "\n",
        "        self.attn_proj = None\n",
        "\n",
        "        if self.aggr == \"attn\":\n",
        "            ############# Your code here #############\n",
        "            ## (~1 line of code)\n",
        "            ## Note:\n",
        "            ## 1. Initialize self.attn_proj here.\n",
        "            ## 2. You should use nn.Sequential for self.attn_proj\n",
        "            ## 3. nn.Linear and nn.Tanh are useful.\n",
        "            ## 4. You can create a vector parameter by using:\n",
        "            ## nn.Linear(some_size, 1, bias=False)\n",
        "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
        "            ## 6. You can assume we only have one \"head\" for the attention.\n",
        "            ## 7. We recommend you to implement the mean aggregation first. After \n",
        "            ## the mean aggregation works well in the training, then you can \n",
        "            ## implement this part.\n",
        "\n",
        "\n",
        "            ##########################################\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        super(HeteroConvWrapper, self).reset_parameters()\n",
        "        if self.aggr == \"attn\":\n",
        "            for layer in self.attn_proj.children():\n",
        "                layer.reset_parameters()\n",
        "    \n",
        "    def forward(self, node_features, edge_indices):\n",
        "        message_type_emb = {}\n",
        "        for message_key, message_type in edge_indices.items():\n",
        "            src_type, edge_type, dst_type = message_key\n",
        "            node_feature_src = node_features[src_type]\n",
        "            node_feature_dst = node_features[dst_type]\n",
        "            edge_index = edge_indices[message_key]\n",
        "            message_type_emb[message_key] = (\n",
        "                self.convs[message_key](\n",
        "                    node_feature_src,\n",
        "                    node_feature_dst,\n",
        "                    edge_index,\n",
        "                )\n",
        "            )\n",
        "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
        "        mapping = {}        \n",
        "        for (src, edge_type, dst), item in message_type_emb.items():\n",
        "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
        "            node_emb[dst].append(item)\n",
        "        self.mapping = mapping\n",
        "        for node_type, embs in node_emb.items():\n",
        "            if len(embs) == 1:\n",
        "                node_emb[node_type] = embs[0]\n",
        "            else:\n",
        "                node_emb[node_type] = self.aggregate(embs)\n",
        "        return node_emb\n",
        "    \n",
        "    def aggregate(self, xs):\n",
        "        # TODO: Implement this function that aggregates all message type results.\n",
        "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
        "        # type aggregation results.\n",
        "\n",
        "        if self.aggr == \"mean\":\n",
        "\n",
        "            ############# Your code here #############\n",
        "            ## (~2 lines of code)\n",
        "\n",
        "\n",
        "            ##########################################\n",
        "\n",
        "        elif self.aggr == \"attn\":\n",
        "\n",
        "            ############# Your code here #############\n",
        "            ## (~10 lines of code)\n",
        "            ## Note:\n",
        "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
        "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
        "            ## to backpropagate etc. in the model. We will use it to see how much \n",
        "            ## attention the layer pays on different message types.\n",
        "            ## 2. torch.softmax and torch.cat are useful.\n",
        "            ## 3. You might need to reshape the tensors by using the \n",
        "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
        "\n",
        "\n",
        "            ##########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn_pnCOKJw-d"
      },
      "source": [
        "## Initialize Heterogeneous GNN Layers\n",
        "\n",
        "Now let's initialize the Heterogeneous GNN Layers. Different from homogeneous graph case, heterogeneous case can be a little bit complex.\n",
        "\n",
        "In general, we need to create a dictionary of `HeteroGNNConv` layers where the keys are message types.\n",
        "\n",
        "* To get all message types, `deepsnap.hetero_graph.HeteroGraph.message_types` is useful.\n",
        "* If we are initializing the first conv layers, we need to get the feature dimension of each node type. Using `deepsnap.hetero_graph.HeteroGraph.num_node_features(node_type)` will return the node feature dimension of `node_type`. In this function, we will set each `HeteroGNNConv` `out_channels` to be `hidden_size`.\n",
        "* If we are not initializing the first conv layers, all node types will have the same embedding dimension `hidden_size` and we still set `HeteroGNNConv` `out_channels` to be `hidden_size` for simplicity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBImHClJzf4"
      },
      "source": [
        "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
        "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
        "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
        "    # object and the `conv` is the `HeteroGNNConv`.\n",
        "\n",
        "    convs = {}\n",
        "\n",
        "    ############# Your code here #############\n",
        "    ## (~9 lines of code)\n",
        "\n",
        "\n",
        "    ##########################################\n",
        "    \n",
        "    return convs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U39dX8EpJ3FG"
      },
      "source": [
        "## HeteroGNN\n",
        "\n",
        "Now we will make a simple HeteroGNN model which contains only two `HeteroGNNWrapperConv` layers.\n",
        "\n",
        "For the forward function in `HeteroGNN`, the model is going to be run as following:\n",
        "\n",
        "$\\text{self.convs1} \\rightarrow \\text{self.bns1} \\rightarrow \\text{self.relus1} \\rightarrow \\text{self.convs2} \\rightarrow \\text{self.bns2} \\rightarrow \\text{self.relus2} \\rightarrow \\text{self.post_mps}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rplknA8aJ6J5"
      },
      "source": [
        "class HeteroGNN(torch.nn.Module):\n",
        "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
        "        super(HeteroGNN, self).__init__()\n",
        "\n",
        "        self.aggr = aggr\n",
        "        self.hidden_size = args['hidden_size']\n",
        "\n",
        "        self.convs1 = None\n",
        "        self.convs2 = None\n",
        "\n",
        "        self.bns1 = nn.ModuleDict()\n",
        "        self.bns2 = nn.ModuleDict()\n",
        "        self.relus1 = nn.ModuleDict()\n",
        "        self.relus2 = nn.ModuleDict()\n",
        "        self.post_mps = nn.ModuleDict()\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~10 lines of code)\n",
        "        ## Note:\n",
        "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
        "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
        "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
        "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
        "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
        "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
        "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
        "        ## where the `out_features` is the number of classes for that node type.\n",
        "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
        "        ## useful.\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "    def forward(self, node_feature, edge_index):\n",
        "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
        "        # a dictionary of tensors where keys are node types and values are \n",
        "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
        "        # tensors where keys are message types and values are corresponding\n",
        "        # edge index tensors (with respect to each message type).\n",
        "\n",
        "        x = node_feature\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~7 lines of code)\n",
        "        ## Note:\n",
        "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def loss(self, preds, y, indices):\n",
        "        \n",
        "        loss = 0\n",
        "        loss_func = F.cross_entropy\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~3 lines of code)\n",
        "        ## Note:\n",
        "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
        "        ## 2. Loss need to be computed with respect to the given index\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e7q_hUJ8zB"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Here we provide you with the functions to train and test. You only need to implement one line of code here.\n",
        "\n",
        "**Please do not modify other parts in `train` and `test` for grading purposes.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI5Hl_5TJ_YL"
      },
      "source": [
        "def train(model, optimizer, hetero_graph, train_idx):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
        "\n",
        "    loss = None\n",
        "\n",
        "    ############# Your code here #############\n",
        "    ## Note:\n",
        "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
        "    ## 2. Compute the loss here\n",
        "\n",
        "\n",
        "    ##########################################\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, graph, indices, best_model=None, best_val=0):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    for index in indices:\n",
        "        preds = model(graph.node_feature, graph.edge_index)\n",
        "        num_node_types = 0\n",
        "        micro = 0\n",
        "        macro = 0\n",
        "        for node_type in preds:\n",
        "            idx = index[node_type]\n",
        "            pred = preds[node_type][idx]\n",
        "            pred = pred.max(1)[1]\n",
        "            label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
        "            pred_np = pred.cpu().numpy()\n",
        "            micro = f1_score(label_np, pred_np, average='micro')\n",
        "            macro = f1_score(label_np, pred_np, average='macro')\n",
        "            num_node_types += 1\n",
        "        # Averaging f1 score might not make sense, but in our example we only\n",
        "        # have one node type\n",
        "        micro /= num_node_types\n",
        "        macro /= num_node_types\n",
        "        accs.append((micro, macro))\n",
        "    if accs[1][0] > best_val:\n",
        "        best_val = accs[1][0]\n",
        "        best_model = copy.deepcopy(model)\n",
        "    return accs, best_model, best_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpNz9B5AKBUU"
      },
      "source": [
        "# Please do not change the following parameters\n",
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'hidden_size': 64,\n",
        "    'epochs': 100,\n",
        "    'weight_decay': 1e-5,\n",
        "    'lr': 0.003,\n",
        "    'attn_size': 32,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRHbWD4hKED8"
      },
      "source": [
        "## Dataset and Preprocessing\n",
        "\n",
        "In the next, we will load the data and create a tensor backend (without a NetworkX graph) `deepsnap.hetero_graph.HeteroGraph` object.\n",
        "\n",
        "We will use the `ACM(3025)` dataset in our node property prediction task, which is proposed in **HAN** ([Wang et al. (2019)](https://arxiv.org/abs/1903.07293)) and our dataset is extracted from [DGL](https://www.dgl.ai/)'s [ACM.mat](https://data.dgl.ai/dataset/ACM.mat).\n",
        "\n",
        "The original ACM dataset has three node types and two edge (relation) types. For simplicity, we simplify the heterogeneous graph to one node type and two edge types (shown below). This means that in our heterogeneous graph, we have one node type (paper) and two message types (paper, author, paper) and (paper, subject, paper).\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img src=\"http://web.stanford.edu/class/cs224w/images/colab4/cs224w-acm.png\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJy03_IsKGh6"
      },
      "source": [
        "print(\"Device: {}\".format(args['device']))\n",
        "\n",
        "# Load the data\n",
        "data = torch.load(\"acm.pkl\")\n",
        "\n",
        "# Message types\n",
        "message_type_1 = (\"paper\", \"author\", \"paper\")\n",
        "message_type_2 = (\"paper\", \"subject\", \"paper\")\n",
        "\n",
        "# Dictionary of edge indices\n",
        "edge_index = {}\n",
        "edge_index[message_type_1] = data['pap']\n",
        "edge_index[message_type_2] = data['psp']\n",
        "\n",
        "# Dictionary of node features\n",
        "node_feature = {}\n",
        "node_feature[\"paper\"] = data['feature']\n",
        "\n",
        "# Dictionary of node labels\n",
        "node_label = {}\n",
        "node_label[\"paper\"] = data['label']\n",
        "\n",
        "# Load the train, validation and test indices\n",
        "train_idx = {\"paper\": data['train_idx'].to(args['device'])}\n",
        "val_idx = {\"paper\": data['val_idx'].to(args['device'])}\n",
        "test_idx = {\"paper\": data['test_idx'].to(args['device'])}\n",
        "\n",
        "# Construct a deepsnap tensor backend HeteroGraph\n",
        "hetero_graph = HeteroGraph(\n",
        "    node_feature=node_feature,\n",
        "    node_label=node_label,\n",
        "    edge_index=edge_index,\n",
        "    directed=True\n",
        ")\n",
        "\n",
        "print(f\"ACM heterogeneous graph: {hetero_graph.num_nodes()} nodes, {hetero_graph.num_edges()} edges\")\n",
        "\n",
        "# Node feature and node label to device\n",
        "for key in hetero_graph.node_feature:\n",
        "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
        "for key in hetero_graph.node_label:\n",
        "    hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
        "\n",
        "# Edge_index to sparse tensor and to device\n",
        "for key in hetero_graph.edge_index:\n",
        "    edge_index = hetero_graph.edge_index[key]\n",
        "    adj = SparseTensor(row=edge_index[0], col=edge_index[1], sparse_sizes=(hetero_graph.num_nodes('paper'), hetero_graph.num_nodes('paper')))\n",
        "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
        "print(hetero_graph.edge_index[message_type_1])\n",
        "print(hetero_graph.edge_index[message_type_2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrmU5-QQKJv6"
      },
      "source": [
        "## Start Training!\n",
        "\n",
        "Now lets start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0HplV9hKMkc"
      },
      "source": [
        "## Training the Mean Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgwfyzLbKOUw"
      },
      "source": [
        "best_model = None\n",
        "best_val = 0\n",
        "\n",
        "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
        "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
        "        f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
        "        f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
        "        f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
        "    )\n",
        "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
        "print(\n",
        "    f\"Best model: \"\n",
        "    f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
        "    f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
        "    f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtkKBI_nKS1T"
      },
      "source": [
        "## Question 2.1: What is the maximum **micro** F1 score you could get for the best_model on test set when using the mean aggregation? (10 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIvw51jMKTvn"
      },
      "source": [
        "## Question 2.2: What is the maximum **macro** F1 score you could get for the best_model on test set when using the mean aggregation? (10 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBiYvwcuKd0z"
      },
      "source": [
        "## Training the Attention Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6na5zyQKfvi"
      },
      "source": [
        "best_model = None\n",
        "best_val = 0\n",
        "\n",
        "output_size = hetero_graph.num_node_labels('paper')\n",
        "model = HeteroGNN(hetero_graph, args, aggr=\"attn\").to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
        "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
        "        f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
        "        f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
        "        f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
        "    )\n",
        "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
        "print(\n",
        "    f\"Best model: \"\n",
        "    f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
        "    f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
        "    f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtAhFLQQKgbl"
      },
      "source": [
        "## Question 2.3: What is the maximum **micro** F1 score you could get for the best_model on test set when using the attention aggregation? (4 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cnsMGbsqJG_"
      },
      "source": [
        "## Question 2.4: What is the maximum **macro** F1 score you could get for the best_model on test set when using the attention aggregation? (4 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQgx5y4UqMHH"
      },
      "source": [
        "## Attention for each Message Type\n",
        "\n",
        "Through message type level attention we can learn that which message type is more important to which layer.\n",
        "\n",
        "Here we will print out and show that each layer pay how much attention on each message type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvK58gijqN_C"
      },
      "source": [
        "if model.convs1.alpha is not None and model.convs2.alpha is not None:\n",
        "    for idx, message_type in model.convs1.mapping.items():\n",
        "        print(f\"Layer 1 has attention {model.convs1.alpha[idx]} on message type {message_type}\")\n",
        "    for idx, message_type in model.convs2.mapping.items():\n",
        "        print(f\"Layer 2 has attention {model.convs2.alpha[idx]} on message type {message_type}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "# Submission\n",
        "\n",
        "In order to get credit, you must go submit your answers on Gradescope.\n",
        "\n",
        "Also, you need to submit the `ipynb` file of Colab 4, by clicking `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
      ]
    }
  ]
}